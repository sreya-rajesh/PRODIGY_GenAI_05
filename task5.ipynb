{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY00N-bonqS3",
        "outputId": "954e5d68-ab4c-43f5-d135-b169ff9bec09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_from_url(url, max_size=512):\n",
        "    \"\"\"Load image from URL and preprocess it\"\"\"\n",
        "    response = requests.get(url)\n",
        "    image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "\n",
        "    # Resize image while maintaining aspect ratio\n",
        "    if max(image.size) > max_size:\n",
        "        image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "PLLDyLCmn2lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_to_tensor(image, device):\n",
        "    \"\"\"Convert PIL image to tensor\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                           std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Add batch dimension\n",
        "    tensor = transform(image).unsqueeze(0)\n",
        "    return tensor.to(device)"
      ],
      "metadata": {
        "id": "c0Edbf-joApc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_image(tensor):\n",
        "    \"\"\"Convert tensor back to PIL image for visualization\"\"\"\n",
        "    # Remove batch dimension\n",
        "    tensor = tensor.squeeze(0).cpu()\n",
        "\n",
        "    # Denormalize\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    tensor = tensor * std + mean\n",
        "\n",
        "    # Clamp values to [0, 1]\n",
        "    tensor = torch.clamp(tensor, 0, 1)\n",
        "\n",
        "    # Convert to PIL image\n",
        "    transform = transforms.ToPILImage()\n",
        "    image = transform(tensor)\n",
        "    return image"
      ],
      "metadata": {
        "id": "xXvY_qlZoEiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGFeatureExtractor(nn.Module):\n",
        "    \"\"\"VGG19 feature extractor for style transfer\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(VGGFeatureExtractor, self).__init__()\n",
        "\n",
        "        # Load pre-trained VGG19\n",
        "        vgg = models.vgg19(pretrained=True).features\n",
        "\n",
        "        # We only need the convolutional layers\n",
        "        self.features = vgg\n",
        "\n",
        "        # Freeze the parameters\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Define which layers to use for content and style\n",
        "        # Content: conv4_2 (relu4_2)\n",
        "        # Style: conv1_1, conv2_1, conv3_1, conv4_1, conv5_1\n",
        "        self.content_layers = ['21']  # conv4_2\n",
        "        self.style_layers = ['0', '5', '10', '19', '28']  # conv1_1, conv2_1, conv3_1, conv4_1, conv5_1\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Extract features from specified layers\"\"\"\n",
        "        content_features = {}\n",
        "        style_features = {}\n",
        "\n",
        "        for name, layer in self.features._modules.items():\n",
        "            x = layer(x)\n",
        "\n",
        "            if name in self.content_layers:\n",
        "                content_features[name] = x\n",
        "\n",
        "            if name in self.style_layers:\n",
        "                style_features[name] = x\n",
        "\n",
        "        return content_features, style_features\n"
      ],
      "metadata": {
        "id": "g753D4LToI05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def content_loss(target_features, content_features):\n",
        "    \"\"\"Calculate content loss\"\"\"\n",
        "    loss = 0\n",
        "    for layer in content_features:\n",
        "        loss += torch.mean((target_features[layer] - content_features[layer]) ** 2)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "4BxnJQxroWcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gram_matrix(tensor):\n",
        "    \"\"\"Calculate Gram matrix for style loss\"\"\"\n",
        "    batch_size, channels, height, width = tensor.size()\n",
        "\n",
        "    # Reshape tensor to (batch_size, channels, height*width)\n",
        "    tensor = tensor.view(batch_size, channels, height * width)\n",
        "\n",
        "    # Calculate Gram matrix\n",
        "    gram = torch.bmm(tensor, tensor.transpose(1, 2))\n",
        "\n",
        "    # Normalize by number of elements\n",
        "    gram = gram / (channels * height * width)\n",
        "\n",
        "    return gram\n"
      ],
      "metadata": {
        "id": "zONqe2Jzoerx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def style_loss(target_features, style_features):\n",
        "    \"\"\"Calculate style loss using Gram matrices\"\"\"\n",
        "    loss = 0\n",
        "    for layer in style_features:\n",
        "        target_gram = gram_matrix(target_features[layer])\n",
        "        style_gram = gram_matrix(style_features[layer])\n",
        "        loss += torch.mean((target_gram - style_gram) ** 2)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "4YJIRfR5oiNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Get image URLs from user\n",
        "    print(\"Neural Style Transfer\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    content_url = input(\"Enter the URL of your content image: \").strip()\n",
        "    style_url = input(\"Enter the URL of your style image: \").strip()\n",
        "\n",
        "    if not content_url or not style_url:\n",
        "        print(\"Error: Both content and style image URLs are required!\")\n",
        "        return\n",
        "\n",
        "    # Get parameters from user (with defaults)\n",
        "    print(\"\\nOptional parameters (press Enter for defaults):\")\n",
        "\n",
        "    content_weight_input = input(f\"Content weight (default: 1e4): \").strip()\n",
        "    content_weight = float(content_weight_input) if content_weight_input else 1e4\n",
        "\n",
        "    style_weight_input = input(f\"Style weight (default: 1e6): \").strip()\n",
        "    style_weight = float(style_weight_input) if style_weight_input else 1e6\n",
        "\n",
        "    learning_rate_input = input(f\"Learning rate (default: 0.01): \").strip()\n",
        "    learning_rate = float(learning_rate_input) if learning_rate_input else 0.01\n",
        "\n",
        "    num_iterations_input = input(f\"Number of iterations (default: 500): \").strip()\n",
        "    num_iterations = int(num_iterations_input) if num_iterations_input else 500\n",
        "\n",
        "    show_every_input = input(f\"Show progress every N iterations (default: 50): \").strip()\n",
        "    show_every = int(show_every_input) if show_every_input else 50\n",
        "\n",
        "    print(f\"\\nUsing parameters:\")\n",
        "    print(f\"Content weight: {content_weight}\")\n",
        "    print(f\"Style weight: {style_weight}\")\n",
        "    print(f\"Learning rate: {learning_rate}\")\n",
        "    print(f\"Iterations: {num_iterations}\")\n",
        "    print(f\"Show progress every: {show_every} iterations\")\n",
        "\n",
        "    # Load images\n",
        "    print(\"\\nLoading images...\")\n",
        "    try:\n",
        "        content_image = load_image_from_url(content_url)\n",
        "        print(\"âœ“ Content image loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading content image: {e}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        style_image = load_image_from_url(style_url)\n",
        "        print(\"âœ“ Style image loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading style image: {e}\")\n",
        "        return\n",
        "\n",
        "    # Convert images to tensors\n",
        "    content_tensor = image_to_tensor(content_image, device)\n",
        "    style_tensor = image_to_tensor(style_image, device)\n",
        "\n",
        "    # Resize style image to match content image size\n",
        "    style_tensor = nn.functional.interpolate(style_tensor, size=content_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "    # Initialize the target image (start with content image)\n",
        "    target_tensor = content_tensor.clone().requires_grad_(True)\n",
        "\n",
        "    # Initialize the feature extractor\n",
        "    print(\"Loading VGG19 model...\")\n",
        "    vgg = VGGFeatureExtractor().to(device)\n",
        "    vgg.eval()\n",
        "\n",
        "    # Extract features from content and style images\n",
        "    print(\"Extracting features...\")\n",
        "    with torch.no_grad():\n",
        "        content_features, _ = vgg(content_tensor)\n",
        "        _, style_features = vgg(style_tensor)\n",
        "\n",
        "    # Setup optimizer\n",
        "    optimizer = optim.Adam([target_tensor], lr=learning_rate)\n",
        "\n",
        "    print(\"\\nStarting style transfer...\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Style transfer optimization loop\n",
        "    for iteration in range(num_iterations):\n",
        "        # Forward pass\n",
        "        target_content_features, target_style_features = vgg(target_tensor)\n",
        "\n",
        "        # Calculate losses\n",
        "        c_loss = content_loss(target_content_features, content_features)\n",
        "        s_loss = style_loss(target_style_features, style_features)\n",
        "        total_loss = content_weight * c_loss + style_weight * s_loss\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress\n",
        "        if iteration % show_every == 0:\n",
        "            print(f\"Iteration {iteration:4d} | Total Loss: {total_loss.item():.2f} | \"\n",
        "                  f\"Content Loss: {c_loss.item():.2f} | Style Loss: {s_loss.item():.2f}\")\n",
        "\n",
        "    print(\"\\nStyle transfer completed!\")\n",
        "\n",
        "    # Convert final result to image\n",
        "    print(\"Displaying final result...\")\n",
        "    final_image = tensor_to_image(target_tensor)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    print(\"âœ“ Final stylized image displayed!\")\n",
        "    plt.imshow(final_image)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Final Stylized Image\")\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "If59ZE2FthR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqJzwyBbpacQ",
        "outputId": "162427b9-9e4d-4d62-b499-8edd3bb98549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Style Transfer\n",
            "==================================================\n",
            "Enter the URL of your content image: https://images.unsplash.com/photo-1544005313-94ddf0286df2?ixlib=rb-4.0.3&auto=format&fit=crop&w=800&q=60\n",
            "Enter the URL of your style image: https://upload.wikimedia.org/wikipedia/commons/f/f4/The_Scream.jpg\n",
            "\n",
            "Optional parameters (press Enter for defaults):\n",
            "Content weight (default: 1e4): \n",
            "Style weight (default: 1e6): \n",
            "Learning rate (default: 0.01): \n",
            "Number of iterations (default: 500): \n",
            "Show progress every N iterations (default: 50): \n",
            "\n",
            "Using parameters:\n",
            "Content weight: 10000.0\n",
            "Style weight: 1000000.0\n",
            "Learning rate: 0.01\n",
            "Iterations: 500\n",
            "Show progress every: 50 iterations\n",
            "\n",
            "Loading images...\n",
            "âœ“ Content image loaded successfully\n",
            "Error loading style image: cannot identify image file <_io.BytesIO object at 0x794551d330b0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "KoX5ohpjpbdz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}